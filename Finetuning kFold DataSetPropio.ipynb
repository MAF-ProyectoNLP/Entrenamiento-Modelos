{"cells":[{"cell_type":"markdown","source":["# En este colab se finetunnea el modelo entrenado para entailment, sobre un dataset propio de saludos y ayuda, utilizando kfold.\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MAF-ProyectoNLP/Entrenamiento-Modelos/blob/main/Finetuning%20kFold%20DataSetPropio.ipynb)\n"],"metadata":{"id":"KmGGpvLn9LUu"}},{"cell_type":"markdown","source":["*0*. Se instalan las bibliotecas necesarias"],"metadata":{"id":"NT8_H1AYDyAs"}},{"cell_type":"code","execution_count":40,"metadata":{"id":"CRDnzcgY1A7-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688397321820,"user_tz":180,"elapsed":12337,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"3508f51f-cc51-46f2-8049-22977520e3f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}],"source":["!pip install datasets\n","!pip install transformers"]},{"cell_type":"markdown","source":["*1*. Se importan las dependencias"],"metadata":{"id":"u3t6snn6D4zJ"}},{"cell_type":"code","execution_count":41,"metadata":{"id":"d--CVfVu01s7","executionInfo":{"status":"ok","timestamp":1688397321821,"user_tz":180,"elapsed":25,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import datasets\n","import numpy as np\n","import random\n","import pandas as pd\n","import gc\n","\n","from transformers import  RobertaForSequenceClassification, RobertaConfig, RobertaTokenizer\n","\n","from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n","\n","from transformers import AdamW\n","\n","from sklearn.model_selection import KFold\n","\n","from pydrive.auth import GoogleAuth\n","from google.colab import drive\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"otmf0mpEXZzW","executionInfo":{"status":"ok","timestamp":1688397321821,"user_tz":180,"elapsed":23,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"outputs":[],"source":["torch.cuda.empty_cache()\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"markdown","source":["*2*. Se descarga el modelo entrenado, y se define la carpeta donde almacenar el modelo final"],"metadata":{"id":"zNfspHOUEPcs"}},{"cell_type":"code","source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","!mkdir \"train\"\n","file_id = '1jQq-kBYyn20G5JYmAqSF9WIwzBO8Sq5C'\n","download = drive.CreateFile({'id': file_id})\n","download.GetContentFile('train/config.json')\n","file_id ='19b0W3oVzq0UQvkZySJqjw-LwsVgqijqr'\n","download = drive.CreateFile({'id': file_id})\n","download.GetContentFile('train/pytorch_model.bin')"],"metadata":{"id":"rHsz50sflOur","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688397399444,"user_tz":180,"elapsed":77645,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"db184223-b976-48a9-b75a-40005e13a043"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘train’: File exists\n"]}]},{"cell_type":"code","source":["carpeta='/content/drive/My Drive/Roberta-Large-BNE-TE-Kfold/kfold/'"],"metadata":{"id":"g3iUdh4NM3s1","executionInfo":{"status":"ok","timestamp":1688397399446,"user_tz":180,"elapsed":55,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["MODEL_TYPE = \"PlanTL-GOB-ES/roberta-large-bne-te\""],"metadata":{"id":"fnLZuUcrI0kE","executionInfo":{"status":"ok","timestamp":1688397399447,"user_tz":180,"elapsed":54,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["MAX_LEN=256\n","BATCH_SIZE = 8\n","NUM_CORES=1\n","L_RATE = 1e-5\n","\n","NUM_EPOCHS = 5\n","torch.manual_seed(555)"],"metadata":{"id":"AzG0BlOALzt9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688397399447,"user_tz":180,"elapsed":53,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"7f84ada4-346f-4811-f8a5-a9176e5ff116"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f7b3051e570>"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["dataset_conversaciones='/content/Entrenamiento-y-Validacion/dataset_propio_validation_set.csv'"],"metadata":{"id":"GjkZD52SzA2I","executionInfo":{"status":"ok","timestamp":1688397399448,"user_tz":180,"elapsed":46,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["Se define el kfold, dividiendo el dataset en 5"],"metadata":{"id":"qI-GZyAmP3JW"}},{"cell_type":"code","source":["kfold=KFold(n_splits=5,shuffle=True)"],"metadata":{"id":"Scj8Gs_eP2T0","executionInfo":{"status":"ok","timestamp":1688397399449,"user_tz":180,"elapsed":47,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["*3*. TestDataset and CompDataset son las clases usadas para la tokenización de la frases, CompDataset se utiliza para entrenar (devuelve también el label), y TestDataset es utilizada para tokenizar las frases para validación"],"metadata":{"id":"XlcjRdGoEWqK"}},{"cell_type":"code","source":["class TestDataset(Dataset):\n","\n","    def __init__(self, df):\n","        self.df_data = df\n","\n","    def __getitem__(self, index):\n","        sentence1 = self.df_data.loc[index, 'premise']\n","        sentence2 = self.df_data.loc[index, 'hypothesis']\n","\n","        encoded_dict = tokenizer.encode_plus(\n","                    sentence1, sentence2,\n","                    add_special_tokens = True,\n","                    max_length = MAX_LEN,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,\n","                    return_tensors = 'pt',\n","                    padding=\"max_length\",\n","                    truncation=True\n","               )\n","\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","\n","        sample = (padded_token_list, att_mask)\n","        return sample\n","\n","    def __len__(self):\n","        return len(self.df_data)"],"metadata":{"id":"nUDPsij_GeIh","executionInfo":{"status":"ok","timestamp":1688397399454,"user_tz":180,"elapsed":51,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","execution_count":50,"metadata":{"id":"go3TdSDpDMGz","executionInfo":{"status":"ok","timestamp":1688397399456,"user_tz":180,"elapsed":52,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"outputs":[],"source":["class CompDataset(Dataset):\n","\n","    def __init__(self, df):\n","        self.df_data = df\n","\n","    def __getitem__(self, index):\n","        sentence1 = self.df_data.loc[index, 'premise']\n","        sentence2 = self.df_data.loc[index, 'hypothesis']\n","\n","        encoded_dict = tokenizer.encode_plus(\n","                    sentence1, sentence2,\n","                    add_special_tokens = True,\n","                    max_length = MAX_LEN,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,\n","                    return_tensors = 'pt',\n","                    padding=\"max_length\",\n","                    truncation=True\n","               )\n","\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","\n","        target = torch.tensor(self.df_data.loc[index, 'label'])\n","\n","        sample = (padded_token_list, att_mask, target)\n","        return sample\n","\n","    def __len__(self):\n","        return len(self.df_data)"]},{"cell_type":"markdown","source":["*4*. Se carga el modelo"],"metadata":{"id":"R6bCpDqsFMSr"}},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained(MODEL_TYPE)"],"metadata":{"id":"-BuTuxVaI9k6","executionInfo":{"status":"ok","timestamp":1688397400017,"user_tz":180,"elapsed":613,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["model_parameters =  RobertaForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels=3)\n","config = RobertaConfig(\n","                vocab_size= model_parameters.config.vocab_size,\n","                hidden_size= model_parameters.config.hidden_size,\n","                num_hidden_layers= model_parameters.config.num_hidden_layers,\n","                num_attention_heads=model_parameters.config.num_attention_heads,\n","                intermediate_size=model_parameters.config.intermediate_size,\n","                hidden_act=model_parameters.config.hidden_act,\n","                hidden_dropout_prob=model_parameters.config.hidden_dropout_prob,\n","                attention_probs_dropout_prob=model_parameters.config.attention_probs_dropout_prob,\n","                max_position_embeddings=model_parameters.config.max_position_embeddings,\n","                type_vocab_size=model_parameters.config.type_vocab_size,\n","                initializer_range=model_parameters.config.initializer_range,\n","            )\n","\n","config.num_labels = 3\n","\n","print(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUFsW97lOlph","executionInfo":{"status":"ok","timestamp":1688397412021,"user_tz":180,"elapsed":12007,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"514ae934-14ad-4a3c-91b9-064c88683da7"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["RobertaConfig {\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.30.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50262\n","}\n","\n"]}]},{"cell_type":"code","source":["model = RobertaForSequenceClassification(config)\n","model = RobertaForSequenceClassification.from_pretrained('/content/train/')"],"metadata":{"id":"e8xHRuYaOrge","executionInfo":{"status":"ok","timestamp":1688397438073,"user_tz":180,"elapsed":26083,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["print(model.roberta.embeddings.word_embeddings.weight.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEt2lq1UOtI4","executionInfo":{"status":"ok","timestamp":1688397438075,"user_tz":180,"elapsed":44,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"4ebba480-4a48-4517-f0b1-7bc54f62f625"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([50262, 1024])\n"]}]},{"cell_type":"code","source":["model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKVCAlu8OtfN","executionInfo":{"status":"ok","timestamp":1688397438076,"user_tz":180,"elapsed":40,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"c7d489b6-beea-4472-d374-da8237359b8a"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50262, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-23): 24 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.0, inplace=False)\n","    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","source":["*5*. Se define un conjunto de frases para validar el algoritmo sobre conversaciones de prueba"],"metadata":{"id":"DPybyDwRFRq2"}},{"cell_type":"code","source":["!git clone \"https://github.com/MAF-ProyectoNLP/Entrenamiento-y-Validacion\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyxXTsgh21Is","executionInfo":{"status":"ok","timestamp":1688397438683,"user_tz":180,"elapsed":642,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"ffcc6c20-361d-4ca1-ae3e-21b92ec0c93a"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Entrenamiento-y-Validacion' already exists and is not an empty directory.\n"]}]},{"cell_type":"markdown","source":["*6*. Entrenamiento"],"metadata":{"id":"4agqDO-0rI2k"}},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(),\n","              lr = L_RATE,\n","              eps = 1e-8\n","            )"],"metadata":{"id":"EUEqxvtyqDCu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688397438684,"user_tz":180,"elapsed":21,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"9d97c3e9-1632-4e5c-bd97-ac316cf03f62"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["df = pd.read_csv(dataset_conversaciones, encoding='Windows-1252', delimiter=\",\")\n","df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_I1x746AMpJK","executionInfo":{"status":"ok","timestamp":1688397438685,"user_tz":180,"elapsed":18,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"c07c6b9c-8cdc-4609-f23d-55d68aed445c"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 148 entries, 0 to 147\n","Data columns (total 4 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   Unnamed: 0  148 non-null    int64 \n"," 1   premise     148 non-null    object\n"," 2   hypothesis  148 non-null    object\n"," 3   label       148 non-null    int64 \n","dtypes: int64(2), object(2)\n","memory usage: 4.8+ KB\n"]}]},{"cell_type":"code","source":["df['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgc53RTXMtG-","executionInfo":{"status":"ok","timestamp":1688397438686,"user_tz":180,"elapsed":14,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"31406c3d-0e73-480f-a6d3-685613dca982"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    87\n","0    61\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["# Set the seed.\n","seed_val = 101\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","dataset = CompDataset(df)\n","\n","for fold,(train_idx,test_idx) in enumerate(kfold.split(dataset)):\n","  print('\\n\\n========Fold no {}========'.format(fold))\n","  train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","  test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n","\n","  train_dataloader = torch.utils.data.DataLoader(\n","                      dataset,\n","                      batch_size=BATCH_SIZE, sampler=train_subsampler, num_workers=NUM_CORES)\n","  val_dataloader = torch.utils.data.DataLoader(\n","                      dataset,\n","                      batch_size=BATCH_SIZE, sampler=test_subsampler,num_workers=NUM_CORES)\n","\n","  print(\"Train \",len(train_dataloader),\" Val \",len(val_dataloader))\n","\n","  # For each epoch...\n","  for epoch in range(0, NUM_EPOCHS):\n","\n","      print(\"\")\n","      print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n","\n","\n","      stacked_val_labels = []\n","      targets_list = []\n","\n","      # ========================================\n","      #               Training\n","      # ========================================\n","\n","      print('Training...')\n","\n","      # put the model into train mode\n","      model.train()\n","\n","      # This turns gradient calculations on and off.\n","      torch.set_grad_enabled(True)\n","\n","\n","      # Reset the total loss for this epoch.\n","      total_train_loss = 0\n","\n","      for i, batch in enumerate(train_dataloader):\n","\n","          train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))\n","\n","          print(train_status, end='\\r')\n","\n","\n","          b_input_ids = batch[0].to(device)\n","          b_input_mask = batch[1].to(device)\n","          b_labels = batch[2].to(device)\n","\n","          model.zero_grad()\n","\n","\n","          outputs = model(b_input_ids,\n","                      attention_mask=b_input_mask,\n","                      labels=b_labels)\n","\n","          # Get the loss from the outputs tuple: (loss, logits)\n","          loss = outputs[0]\n","\n","          # Convert the loss from a torch tensor to a number.\n","          # Calculate the total loss.\n","          total_train_loss = total_train_loss + loss.item()\n","\n","          # Zero the gradients\n","          optimizer.zero_grad()\n","\n","          # Perform a backward pass to calculate the gradients.\n","          loss.backward()\n","\n","\n","          # Clip the norm of the gradients to 1.0.\n","          # This is to help prevent the \"exploding gradients\" problem.\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","\n","\n","          # Use the optimizer to update the weights.\n","\n","          # Optimizer for GPU\n","          optimizer.step()\n","\n","\n","      print('Train loss:' ,total_train_loss)\n","\n","      # ========================================\n","      #               Validation\n","      # ========================================\n","\n","      print('\\nValidation...')\n","\n","      # Put the model in evaluation mode.\n","      model.eval()\n","\n","      # Turn off the gradient calculations.\n","      # This tells the model not to compute or store gradients.\n","      # This step saves memory and speeds up validation.\n","      torch.set_grad_enabled(False)\n","\n","      # Reset the total loss for this epoch.\n","      total_val_loss = 0\n","\n","\n","      for j, batch in enumerate(val_dataloader):\n","\n","          val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))\n","\n","          print(val_status, end='\\r')\n","\n","          b_input_ids = batch[0].to(device)\n","          b_input_mask = batch[1].to(device)\n","          b_labels = batch[2].to(device)\n","\n","\n","          outputs = model(b_input_ids,\n","                  attention_mask=b_input_mask,\n","                  labels=b_labels)\n","\n","          # Get the loss from the outputs tuple: (loss, logits)\n","          loss = outputs[0]\n","\n","          # Convert the loss from a torch tensor to a number.\n","          # Calculate the total loss.\n","          total_val_loss = total_val_loss + loss.item()\n","\n","          # Get the preds\n","          preds = outputs[1]\n","\n","          # Move preds to the CPU\n","          val_preds = preds.detach().cpu().numpy()\n","\n","          # Move the labels to the cpu\n","          targets_np = b_labels.to('cpu').numpy()\n","\n","          # Append the labels to a numpy list\n","          targets_list.extend(targets_np)\n","\n","          if j == 0:  # first batch\n","              stacked_val_preds = val_preds\n","\n","          else:\n","              stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n","\n","\n","      # Calculate the validation accuracy\n","      y_true = targets_list\n","      y_pred = np.argmax(stacked_val_preds, axis=1)\n","\n","\n","      y_true = np.array(y_true)\n","\n","      y_pred[y_pred==2] = 1\n","\n","      val_acc = accuracy_score(y_true, y_pred)\n","\n","      prec_sc = precision_score(y_true, y_pred)\n","      rec_sc = recall_score(y_true, y_pred)\n","\n","      f1_sc = f1_score(y_true, y_pred)\n","\n","\n","      print('Val loss:' ,total_val_loss)\n","      print('Val acc: ', val_acc)\n","      print('\\n\\nPrecision: ', prec_sc)\n","      print('Recall: ', rec_sc)\n","      print('F1: ', f1_sc)\n","\n","\n","      if fold ==4 and epoch ==4:\n","        # Save the Model\n","        model.save_pretrained(carpeta)\n","\n","      # Use the garbage collector to save memory.\n","      gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cISvoKS8Mta8","executionInfo":{"status":"ok","timestamp":1688397944859,"user_tz":180,"elapsed":506183,"user":{"displayName":"Federico Detta","userId":"05190204888392799794"}},"outputId":"cf5661da-5513-44a8-ffea-bb60e7cb7188"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","========Fold no 0========\n","Train  15  Val  4\n","\n","======== Epoch 1 / 5 ========\n","Training...\n","Train loss: 6.924230232834816\n","\n","Validation...\n","Val loss: 3.1304752826690674\n","Val acc:  0.7666666666666667\n","\n","\n","Precision:  0.7777777777777778\n","Recall:  0.8235294117647058\n","F1:  0.7999999999999999\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","Train loss: 2.988842346239835\n","\n","Validation...\n","Val loss: 3.9976515262387693\n","Val acc:  0.8\n","\n","\n","Precision:  0.7894736842105263\n","Recall:  0.8823529411764706\n","F1:  0.8333333333333333\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","Train loss: 1.2445435036206618\n","\n","Validation...\n","Val loss: 2.8522619060240686\n","Val acc:  0.8333333333333334\n","\n","\n","Precision:  0.8\n","Recall:  0.9411764705882353\n","F1:  0.8648648648648648\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","Train loss: 1.0415815755259246\n","\n","Validation...\n","Val loss: 3.059456020593643\n","Val acc:  0.8\n","\n","\n","Precision:  0.7894736842105263\n","Recall:  0.8823529411764706\n","F1:  0.8333333333333333\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","Train loss: 0.017492094659246504\n","\n","Validation...\n","Val loss: 3.1991402862477116\n","Val acc:  0.8\n","\n","\n","Precision:  0.7894736842105263\n","Recall:  0.8823529411764706\n","F1:  0.8333333333333333\n","\n","\n","========Fold no 1========\n","Train  15  Val  4\n","\n","======== Epoch 1 / 5 ========\n","Training...\n","Train loss: 3.7586798628326505\n","\n","Validation...\n","Val loss: 0.009410239523276687\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","Train loss: 1.5310731676290743\n","\n","Validation...\n","Val loss: 0.08045500935986638\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","Train loss: 0.1292031030752696\n","\n","Validation...\n","Val loss: 0.0025500819028820843\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","Train loss: 0.0647265307197813\n","\n","Validation...\n","Val loss: 0.001726038521155715\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","Train loss: 0.6484391586564016\n","\n","Validation...\n","Val loss: 0.0015688588027842343\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","\n","========Fold no 2========\n","Train  15  Val  4\n","\n","======== Epoch 1 / 5 ========\n","Training...\n","Train loss: 0.03563203784869984\n","\n","Validation...\n","Val loss: 0.001083833834854886\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","Train loss: 0.007733321195701137\n","\n","Validation...\n","Val loss: 0.0008888422598829493\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","Train loss: 0.003420375389396213\n","\n","Validation...\n","Val loss: 0.0007715310930507258\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","Train loss: 0.0029640784196089953\n","\n","Validation...\n","Val loss: 0.0006842865113867447\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","Train loss: 0.0025852664111880586\n","\n","Validation...\n","Val loss: 0.0006145954830572009\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","\n","========Fold no 3========\n","Train  15  Val  4\n","\n","======== Epoch 1 / 5 ========\n","Training...\n","Train loss: 0.00228037936904002\n","\n","Validation...\n","Val loss: 0.0005787784612039104\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","Train loss: 0.0020578170369844884\n","\n","Validation...\n","Val loss: 0.0005246023647487164\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","Train loss: 0.0018712198943831027\n","\n","Validation...\n","Val loss: 0.00047935923794284463\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","Train loss: 0.0017126398233813234\n","\n","Validation...\n","Val loss: 0.00043835303222294897\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","Train loss: 0.0015760688547743484\n","\n","Validation...\n","Val loss: 0.00040542792703490704\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","\n","========Fold no 4========\n","Train  15  Val  4\n","\n","======== Epoch 1 / 5 ========\n","Training...\n","Train loss: 0.0014664739210275002\n","\n","Validation...\n","Val loss: 0.0003686464042402804\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","Train loss: 0.0013593087060144171\n","\n","Validation...\n","Val loss: 0.0003434269528952427\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","Train loss: 0.0012659122876357287\n","\n","Validation...\n","Val loss: 0.0003193666343577206\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","Train loss: 0.0011817235572380014\n","\n","Validation...\n","Val loss: 0.00029891193116782233\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","Train loss: 0.0011069914617110044\n","\n","Validation...\n","Val loss: 0.00028326980100246146\n","Val acc:  1.0\n","\n","\n","Precision:  1.0\n","Recall:  1.0\n","F1:  1.0\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/MAF-ProyectoNLP/Validacion-Modelos/blob/main/Entrenamiento%20propio%20-%20validation%20set%20conversaci%C3%B3n.ipynb","timestamp":1688394863819},{"file_id":"1BZsTEJakbFk1m6adxCPShnaTNxyUsbCS","timestamp":1681058421099},{"file_id":"1NOBQags_uOFWyYASyEPbDDPBuU0kwqzg","timestamp":1681057693203},{"file_id":"https://github.com/MAF-ProyectoNLP/Validacion-Modelos/blob/main/BNE-TE%20%2B%20Finentuning%20Saludos_%20evaluaci%C3%B3n%20conversaci%C3%B3n.ipynb","timestamp":1681056720426},{"file_id":"1Nq7geh86MOtowv6w6EL6X8fZBZMZpq2o","timestamp":1680362983238},{"file_id":"16Tjjq9eICtTgp2RGATtMLB2SeARgNbwi","timestamp":1679080973502},{"file_id":"1_pd6QzOfkcqAQqEwaiwze0DYbH3hGmy2","timestamp":1678127873327},{"file_id":"1W-WrD6qptDBRYXe6eYteKHNOSKPYQ41G","timestamp":1674065825428},{"file_id":"1Vj9qdta0mWXcGlmhQi6ENr5YSQ3vF_re","timestamp":1673194755319},{"file_id":"1MZ9lybPOplwQ2owRVfjkiAMATtpJTme8","timestamp":1672190878229},{"file_id":"1i8e6p6bF0KamJxr8hIB0r0Vz_msqD6LL","timestamp":1671802132551},{"file_id":"1H8g_16z54A_ZQLkFAFlxLix1TJiZO9W2","timestamp":1671489887814},{"file_id":"1sBnqmuahCZ7RyAkh5scH3VU4NdI8f88f","timestamp":1671475822398},{"file_id":"1PuvYPOpvsdWftFkqkrYqo1Xc04TEfLN0","timestamp":1671230785987},{"file_id":"1m7Au5A022WDcbz4_AdIcmOnqCx0Ey3D2","timestamp":1670942391340},{"file_id":"1a0BjY73SdKUutqAyOWT8otFGbiQny3H4","timestamp":1670938866684},{"file_id":"1R5ivjMsg5I5ZthPO7lFbnLhM6KUCwRx6","timestamp":1670872522224},{"file_id":"1K8DJYBCAcNc1Up1_SwnF0UrysBzlIYO_","timestamp":1670333392269},{"file_id":"1BXXkeP2N1qj8e4c-IRCluFxRDvMQd8ou","timestamp":1665705882241},{"file_id":"1KEb7KvkixUWgQQmbqo0D4l26SRbXQEPJ","timestamp":1665442019692}],"machine_shape":"hm","gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}